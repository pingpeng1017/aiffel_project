{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74acddc",
   "metadata": {},
   "source": [
    "## **프로젝트: 멋진 작사가 만들기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f61450",
   "metadata": {},
   "source": [
    "### **Step 1. 데이터 읽어오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4db609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import\n",
    "import glob\n",
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e600d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation', \"Baby, can't you read the signs? I won't bore you with the details, baby\", \"I don't even wanna waste your time\", \"Let's just say that maybe\", 'You could help me ease my mind', \"I ain't Mr. Right But if you're looking for fast love\", \"If that's love in your eyes\", \"It's more than enough\"]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "txt_file_path = 'C:/Users/Minjoo Lee/AIFFEL/lyricist/data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = [] \n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus에 담기\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, 'rt', encoding='UTF8') as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18adc46",
   "metadata": {},
   "source": [
    "### **Step 2. 데이터 정제**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7ed573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 정제 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence\n",
    "\n",
    "# 문장이 어떻게 필터링 되는지 확인\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d59a4efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> looking for some education <end>',\n",
       " '<start> made my way into the night <end>',\n",
       " '<start> all that bullshit conversation <end>',\n",
       " '<start> baby , can t you read the signs ? i won t bore you with the details , baby <end>',\n",
       " '<start> i don t even wanna waste your time <end>',\n",
       " '<start> let s just say that maybe <end>',\n",
       " '<start> you could help me ease my mind <end>',\n",
       " '<start> i ain t mr . right but if you re looking for fast love <end>',\n",
       " '<start> if that s love in your eyes <end>',\n",
       " '<start> it s more than enough <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정제 데이터 구축\n",
    "# 정제된 문장을 담을 리스트\n",
    "corpus = []\n",
    "\n",
    "# raw_corpus list에 저장된 문장들을 순서대로 반환하여 sentence에 저장\n",
    "for sentence in raw_corpus:\n",
    "    # 원하지 않는 문장은 건너뛰기\n",
    "    if len(sentence) == 0: continue # 길이가 0\n",
    "    if sentence[-1] == \":\": continue # 문장의 끝이 :\n",
    "    \n",
    "    # preprocess_sentence() 함수를 이용하여 문장을 정제를 하고 담아주기\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과 확인\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af162672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2 304  28 ...   0   0   0]\n",
      " [  2 221  13 ...   0   0   0]\n",
      " [  2  24  17 ...   0   0   0]\n",
      " ...\n",
      " [  2  23  77 ...   0   0   0]\n",
      " [  2  42  26 ...   0   0   0]\n",
      " [  2  23  77 ...   0   0   0]] <keras.preprocessing.text.Tokenizer object at 0x00000161E67B5648>\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 토큰화하는 함수\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장 완성\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # tokenizer를 이용해 corpus를 Tensor로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e3d350",
   "metadata": {},
   "source": [
    "생성된 텐서 데이터를 3번째 행, 10번째 열까지만 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0a4613a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  304   28   99 4811    3    0    0    0    0]\n",
      " [   2  221   13   85  226    6  115    3    0    0]\n",
      " [   2   24   17 1087 2347    3    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22363f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전이 어떻게 구축되었는지 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c112f2",
   "metadata": {},
   "source": [
    "2번 인덱스가 `<start>`여서 모든 행이 2로 시작하는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81b165cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2  304   28   99 4811    3    0    0    0    0    0    0    0    0]\n",
      "[ 304   28   99 4811    3    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4aedf",
   "metadata": {},
   "source": [
    "행 뒤쪽에 0이 많이 나온 부분은 정해진 입력 시퀀스 길이보다 문장이 짧을 경우 0으로 패딩을 채워 넣은 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d60db37",
   "metadata": {},
   "source": [
    "### **Step 3. 평가 데이터셋 분리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8df8f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 분리\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          random_state=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46e349a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140599, 14)\n",
      "Target Train: (140599, 14)\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 크기 확인\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "649b3845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 객체 생성\n",
    "def create_dataset(src_input,tgt_input):\n",
    "    BUFFER_SIZE = len(src_input)\n",
    "    BATCH_SIZE = 256\n",
    "    steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "    VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(enc_train, dec_train)\n",
    "valid_dataset = create_dataset(enc_val, dec_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ca6e3",
   "metadata": {},
   "source": [
    "### **Step 4. 인공지능 만들기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a49c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구현\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.dropout_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.dropout_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "embedding_size = 1024 # 워드 벡터의 차원수를 말하며 단어가 추상적으로 표현되는 크기\n",
    "hidden_size = 2048 # 모델에 얼마나 많은 일꾼을 둘 것인가?\n",
    "dropout_rate = 0.2\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size, dropout_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0ffc725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-2.6376138e-04, -4.6119507e-04,  2.8968582e-04, ...,\n",
       "          2.4202246e-04, -1.0433166e-04, -1.0743681e-04],\n",
       "        [-2.7145414e-05, -7.8982412e-04,  1.1621646e-03, ...,\n",
       "          2.1229715e-04, -6.1762439e-05, -1.3538405e-04],\n",
       "        [ 7.7120661e-05, -8.5416622e-04,  7.7189744e-04, ...,\n",
       "          5.5689760e-04, -3.5853413e-04,  1.6359851e-04],\n",
       "        ...,\n",
       "        [ 1.4803661e-03,  8.1505592e-04, -2.0033547e-04, ...,\n",
       "         -8.1072416e-04, -1.1499242e-03, -1.6644222e-03],\n",
       "        [ 1.1851481e-03,  1.0534307e-03,  1.8646722e-04, ...,\n",
       "         -5.1133573e-04, -6.2646717e-04, -1.3026163e-03],\n",
       "        [ 8.7161199e-04,  1.0852305e-03,  8.5541855e-05, ...,\n",
       "         -4.4379581e-04, -1.5149425e-05, -7.0503535e-04]],\n",
       "\n",
       "       [[-2.6376138e-04, -4.6119507e-04,  2.8968582e-04, ...,\n",
       "          2.4202246e-04, -1.0433166e-04, -1.0743681e-04],\n",
       "        [-3.5065092e-04, -6.3639862e-04,  3.0992139e-04, ...,\n",
       "          1.0990279e-06, -2.9915973e-04, -2.8762501e-04],\n",
       "        [-9.1320928e-04, -1.0234750e-03,  4.0285653e-04, ...,\n",
       "         -6.4236106e-04, -3.2842037e-04, -1.3215056e-04],\n",
       "        ...,\n",
       "        [-8.6135662e-04, -1.2935714e-03, -3.1987135e-03, ...,\n",
       "         -5.9638725e-04,  2.6211292e-03,  3.1596571e-03],\n",
       "        [-7.3845882e-04, -1.2605273e-03, -3.6921822e-03, ...,\n",
       "         -7.5431436e-04,  2.8178210e-03,  3.7861280e-03],\n",
       "        [-5.7092158e-04, -1.2100185e-03, -4.0818104e-03, ...,\n",
       "         -9.4015617e-04,  2.9361844e-03,  4.4059195e-03]],\n",
       "\n",
       "       [[-2.6376138e-04, -4.6119507e-04,  2.8968582e-04, ...,\n",
       "          2.4202246e-04, -1.0433166e-04, -1.0743681e-04],\n",
       "        [-4.5686518e-04, -5.3144485e-04,  5.4481300e-04, ...,\n",
       "          3.2427619e-04,  4.3400284e-04, -3.0577261e-04],\n",
       "        [-8.9247205e-04, -6.6177244e-04,  9.8342844e-04, ...,\n",
       "          2.5269811e-04,  7.9538027e-04, -3.0170576e-04],\n",
       "        ...,\n",
       "        [-1.3915767e-03,  9.4019104e-04, -5.7803141e-04, ...,\n",
       "         -3.5792153e-04,  2.3323628e-03,  3.5662504e-04],\n",
       "        [-1.2653334e-03,  6.0427160e-04, -1.2244927e-03, ...,\n",
       "         -5.8420160e-04,  2.5979283e-03,  1.0431102e-03],\n",
       "        [-1.0934171e-03,  2.9400163e-04, -1.8307306e-03, ...,\n",
       "         -8.2478509e-04,  2.7634394e-03,  1.7503353e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.6376138e-04, -4.6119507e-04,  2.8968582e-04, ...,\n",
       "          2.4202246e-04, -1.0433166e-04, -1.0743681e-04],\n",
       "        [-2.3931720e-04, -6.5718987e-04,  5.5198884e-04, ...,\n",
       "          5.0757173e-04, -3.9698891e-04, -2.4386439e-04],\n",
       "        [-4.4876931e-04, -7.0559577e-04,  5.6499126e-04, ...,\n",
       "          9.6354133e-04, -2.5530026e-04, -9.6088630e-04],\n",
       "        ...,\n",
       "        [-2.3955680e-04, -9.5870689e-04, -1.9959849e-04, ...,\n",
       "         -3.1505544e-05,  3.5149292e-03,  2.0359082e-03],\n",
       "        [-2.1740106e-04, -9.5918233e-04, -9.6707430e-04, ...,\n",
       "         -4.2020157e-04,  3.6416391e-03,  2.7358592e-03],\n",
       "        [-1.6008185e-04, -9.4603875e-04, -1.6604245e-03, ...,\n",
       "         -7.9311372e-04,  3.6983448e-03,  3.4219662e-03]],\n",
       "\n",
       "       [[-2.6376138e-04, -4.6119507e-04,  2.8968582e-04, ...,\n",
       "          2.4202246e-04, -1.0433166e-04, -1.0743681e-04],\n",
       "        [-1.9019179e-04, -5.0838292e-04,  3.4137099e-04, ...,\n",
       "          1.0259201e-04, -2.7097246e-04, -5.5711938e-04],\n",
       "        [-5.1686977e-04, -5.8965216e-04,  9.6420350e-04, ...,\n",
       "          6.3367840e-04, -5.5353524e-04, -1.0956778e-03],\n",
       "        ...,\n",
       "        [-8.4429630e-04, -1.4239518e-03, -1.8110651e-03, ...,\n",
       "          4.7934137e-04,  3.4059808e-04,  4.0526083e-04],\n",
       "        [-9.1039808e-04, -1.3696451e-03, -2.5210534e-03, ...,\n",
       "          1.3988724e-04,  7.5214752e-04,  1.1856715e-03],\n",
       "        [-9.2185463e-04, -1.3282335e-03, -3.1921335e-03, ...,\n",
       "         -2.2237620e-04,  1.0953286e-03,  1.9418763e-03]],\n",
       "\n",
       "       [[-2.6376138e-04, -4.6119507e-04,  2.8968582e-04, ...,\n",
       "          2.4202246e-04, -1.0433166e-04, -1.0743681e-04],\n",
       "        [-6.9114889e-05, -1.1855193e-03,  4.4118837e-04, ...,\n",
       "         -7.0501876e-05, -4.9491116e-04, -2.2820404e-04],\n",
       "        [ 5.3581968e-04, -1.6275932e-03,  7.5824442e-04, ...,\n",
       "         -3.7205219e-04, -3.0216281e-04, -4.0305874e-04],\n",
       "        ...,\n",
       "        [ 3.5405203e-04, -1.3262728e-03, -6.2427507e-04, ...,\n",
       "         -7.7676296e-04, -6.2763039e-04, -9.2647702e-04],\n",
       "        [ 5.7140038e-05, -1.2140732e-03, -9.3824801e-04, ...,\n",
       "         -9.8492310e-04, -1.4264208e-04, -4.5488609e-04],\n",
       "        [-1.7437906e-04, -1.1195632e-03, -1.4162025e-03, ...,\n",
       "         -1.1806210e-03,  2.8327695e-04,  1.7991535e-04]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법\n",
    "for src_sample, tgt_sample in train_dataset.take(1): break\n",
    "    \n",
    "# 불러온 데이터를 모델에 넣어보기\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "008767c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  12289024  \n",
      "                                                                 \n",
      " lstm (LSTM)                 multiple                  25174016  \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               multiple                  33562624  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  24590049  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 95,615,713\n",
      "Trainable params: 95,615,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조 확인\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da30307",
   "metadata": {},
   "source": [
    "모델은 입력 시퀀스의 길이를 모르기 때문에 Output Shape를 특정할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75f7c0a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "549/549 [==============================] - 80s 143ms/step - loss: 3.4340 - val_loss: 3.0322\n",
      "Epoch 2/10\n",
      "549/549 [==============================] - 79s 144ms/step - loss: 2.8617 - val_loss: 2.7554\n",
      "Epoch 3/10\n",
      "549/549 [==============================] - 80s 146ms/step - loss: 2.5407 - val_loss: 2.5736\n",
      "Epoch 4/10\n",
      "549/549 [==============================] - 79s 144ms/step - loss: 2.2545 - val_loss: 2.4413\n",
      "Epoch 5/10\n",
      "549/549 [==============================] - 79s 144ms/step - loss: 2.0020 - val_loss: 2.3522\n",
      "Epoch 6/10\n",
      "549/549 [==============================] - 79s 144ms/step - loss: 1.7830 - val_loss: 2.2896\n",
      "Epoch 7/10\n",
      "549/549 [==============================] - 79s 144ms/step - loss: 1.5999 - val_loss: 2.2494\n",
      "Epoch 8/10\n",
      "549/549 [==============================] - 79s 143ms/step - loss: 1.4487 - val_loss: 2.2308\n",
      "Epoch 9/10\n",
      "549/549 [==============================] - 80s 145ms/step - loss: 1.3272 - val_loss: 2.2276\n",
      "Epoch 10/10\n",
      "549/549 [==============================] - 79s 144ms/step - loss: 1.2326 - val_loss: 2.2373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x161e93b3448>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                     reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(train_dataset, \n",
    "          validation_data=valid_dataset,\n",
    "          batch_size=256,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3dbea",
   "metadata": {},
   "source": [
    "### **Step 5. 평가하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79ce0ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 생성 함수 정의\n",
    "# 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=15):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장 만들기\n",
    "    while True: # 루프를 돌면서 init_sentence에 단어를 하나씩 생성\n",
    "        predict = model(test_tensor) \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated # 최종적으로 모델이 생성한 문장을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a72eff7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you more than i love myself , <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdca7bbe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e1277d",
   "metadata": {},
   "source": [
    "### **프로젝트 회고**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba2c0b",
   "metadata": {},
   "source": [
    "이번 프로젝트에서의 목표는 10 epoch 안에 val_loss 값을 2.2 이하로 줄이는 것이었다. 그러나 초기 val_loss 값이 이미 2.2에 근접한 값이었기 때문에 embedding_size와 hidden_size를 조절하는 것만으로는 val_loss 값을 쉽게 개선할 수 없었다. 따라서, 추가적인 시도를 해봤다. 첫번째로는 model.fit에 다양한 인자를 추가해봤지만, 이는 val_loss 값에 큰 변화를 가져오지 못했다. 그 다음으로는 dropout 레이어를 추가하고 random_state 숫자를 변경해가며 실험해봤는데, 이렇게 함으로써 val_loss 값을 2.2로 달성할 수 있었다. 그러나 마지막 epoch에서 val_loss 값이 증가하기 시작하는 현상을 관찰할 수 있었고, 이는 과대적합이 발생했을 가능성을 의미한다. 따라서, 과대적합을 완화하기 위해 추가적인 조치가 필요할 것으로 판단된다. 모델 구조 변경, 하이퍼파라미터 튜닝, 정규화 등 여러가지를 시도해봤는데도 불구하고 모델의 성능 향상이 크게 이루어지지 않았는데, LSTM을 대체할 다른 모델을 사용해보는 것도 하나의 방법일 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a40d5",
   "metadata": {},
   "source": [
    "#### **새로 알게 된 것!**\n",
    "* 시퀀스(Sequence): 연속된 데이터 요소들이 순서대로 나열된 것\n",
    "* 순환신경망(RNN): 순차적인 데이터를 다루는 인공지능 모델, 이전 단계의 정보를 기억하면서 현재 단계의 입력과 함께 다음 단계의 출력을 계산하는 방식으로 동작\n",
    "* 언어 모델(Language Model): 이전 단어들의 시퀀스가 주어졌을 때, 다음 단어의 확률을 예측하는 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b1d0e",
   "metadata": {},
   "source": [
    "**GPU 설치**  \n",
    "드.디.어. 로컬 환경에 GPU를 성공적으로 설치했다!!! 이번 프로젝트에서 학습 시간이 너무 오래 걸려서 이전에 실패했던 GPU 설치에 다시 도전해봤다. 그 결과로 확실히 클라우드 서비스 대신 로컬에서 GPU를 사용하면 학습 시간이 단축되고 작업을 보다 빠른 속도로 수행할 수 있었다. 이제 GPU를 통해 딥러닝 작업을 로컬에서 더욱 빠르고 효율적으로 진행할 수 있게 되었다 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ba110",
   "metadata": {},
   "source": [
    "**참고한 레퍼런스**\n",
    "<br>[Python] 시퀀스 자료형 #1 리스트(list) https://kukuta.tistory.com/310\n",
    "<br>[인공지능 개념] Tensor란 무엇인가? https://rekt77.tistory.com/102\n",
    "<br>tf.keras.Model - TensorFlow https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
    "<br>GPU 지원 - TensorFlow https://www.tensorflow.org/install/gpu?hl=ko\n",
    "<br>[jupyter notebook]tensorflow-gpu 사용 설정하기 https://hengbokhan.tistory.com/75"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
