{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851df620",
   "metadata": {},
   "source": [
    "## **프로젝트: 멋진 작사가 만들기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c0575a",
   "metadata": {},
   "source": [
    "### **Step 1. 데이터 읽어오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f78bc03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import\n",
    "import glob\n",
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df77bb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation', \"Baby, can't you read the signs? I won't bore you with the details, baby\", \"I don't even wanna waste your time\", \"Let's just say that maybe\", 'You could help me ease my mind', \"I ain't Mr. Right But if you're looking for fast love\", \"If that's love in your eyes\", \"It's more than enough\"]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "txt_file_path = 'C:/Users/Minjoo Lee/AIFFEL/lyricist/data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = [] \n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus에 담기\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, 'rt', encoding='UTF8') as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13df62c6",
   "metadata": {},
   "source": [
    "### **Step 2. 데이터 정제**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c1a9cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 정제 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence\n",
    "\n",
    "# 문장이 어떻게 필터링 되는지 확인\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08573be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> looking for some education <end>',\n",
       " '<start> made my way into the night <end>',\n",
       " '<start> all that bullshit conversation <end>',\n",
       " '<start> baby , can t you read the signs ? i won t bore you with the details , baby <end>',\n",
       " '<start> i don t even wanna waste your time <end>',\n",
       " '<start> let s just say that maybe <end>',\n",
       " '<start> you could help me ease my mind <end>',\n",
       " '<start> i ain t mr . right but if you re looking for fast love <end>',\n",
       " '<start> if that s love in your eyes <end>',\n",
       " '<start> it s more than enough <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정제 데이터 구축\n",
    "# 정제된 문장을 담을 리스트\n",
    "corpus = []\n",
    "\n",
    "# raw_corpus list에 저장된 문장들을 순서대로 반환하여 sentence에 저장\n",
    "for sentence in raw_corpus:\n",
    "    # 원하지 않는 문장은 건너뛰기\n",
    "    if len(sentence) == 0: continue # 길이가 0\n",
    "    if sentence[-1] == \":\": continue # 문장의 끝이 :\n",
    "    \n",
    "    # preprocess_sentence() 함수를 이용하여 문장을 정제를 하고 담아주기\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과 확인\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71527a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2 304  28 ...   0   0   0]\n",
      " [  2 221  13 ...   0   0   0]\n",
      " [  2  24  17 ...   0   0   0]\n",
      " ...\n",
      " [  2  23  77 ...   0   0   0]\n",
      " [  2  42  26 ...   0   0   0]\n",
      " [  2  23  77 ...   0   0   0]] <keras.preprocessing.text.Tokenizer object at 0x0000021701FE7788>\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 토큰화하는 함수\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장 완성\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # tokenizer를 이용해 corpus를 Tensor로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c9198",
   "metadata": {},
   "source": [
    "생성된 텐서 데이터를 3번째 행, 10번째 열까지만 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b41b3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  304   28   99 4811    3    0    0    0    0]\n",
      " [   2  221   13   85  226    6  115    3    0    0]\n",
      " [   2   24   17 1087 2347    3    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd601713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전이 어떻게 구축되었는지 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf51df",
   "metadata": {},
   "source": [
    "2번 인덱스가 `<start>`여서 모든 행이 2로 시작하는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1ba9a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2  304   28   99 4811    3    0    0    0    0    0    0    0    0]\n",
      "[ 304   28   99 4811    3    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553aa78b",
   "metadata": {},
   "source": [
    "행 뒤쪽에 0이 많이 나온 부분은 정해진 입력 시퀀스 길이보다 문장이 짧을 경우 0으로 패딩을 채워 넣은 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca78b7f",
   "metadata": {},
   "source": [
    "### **Step 3. 평가 데이터셋 분리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87d9a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea752181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(256, 14), dtype=tf.int32, name=None), TensorSpec(shape=(256, 14), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 객체 생성\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e91c7a3",
   "metadata": {},
   "source": [
    "### **Step 4. 인공지능 만들기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4757d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구현\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "embedding_size = 512 # 워드 벡터의 차원수를 말하며 단어가 추상적으로 표현되는 크기\n",
    "hidden_size = 2048 # 모델에 얼마나 많은 일꾼을 둘 것인가?\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55fd840c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 1.96383931e-04,  3.06807087e-05, -3.20026011e-04, ...,\n",
       "          1.60722033e-04,  3.85537860e-04, -7.05594794e-05],\n",
       "        [ 4.32939676e-04, -5.30474063e-05, -5.92583965e-04, ...,\n",
       "          9.65326690e-05,  4.65600635e-04,  3.21498965e-06],\n",
       "        [ 6.51538779e-04,  1.51049739e-04, -5.17855748e-04, ...,\n",
       "          2.70104356e-04,  5.20643138e-04,  4.49553336e-05],\n",
       "        ...,\n",
       "        [-1.16748262e-04, -3.91560374e-04, -1.09337980e-03, ...,\n",
       "          1.65157788e-03,  3.31841479e-03,  7.40238174e-04],\n",
       "        [-1.83879092e-04, -6.69284374e-04, -1.39642623e-03, ...,\n",
       "          1.94959925e-03,  3.65324854e-03,  1.10658945e-03],\n",
       "        [-2.41904272e-04, -9.30300623e-04, -1.70498784e-03, ...,\n",
       "          2.22853385e-03,  3.94132733e-03,  1.45016098e-03]],\n",
       "\n",
       "       [[ 1.96383931e-04,  3.06807087e-05, -3.20026011e-04, ...,\n",
       "          1.60722033e-04,  3.85537860e-04, -7.05594794e-05],\n",
       "        [ 9.39750971e-05, -2.05085133e-04, -4.22945304e-04, ...,\n",
       "          5.38946129e-04,  5.77497238e-04,  6.31291769e-05],\n",
       "        [ 5.22001901e-05, -2.67317868e-04, -4.99941176e-04, ...,\n",
       "          5.22723247e-04,  7.89488549e-04,  4.67975391e-04],\n",
       "        ...,\n",
       "        [-7.59929884e-04, -1.57158996e-04, -1.90156139e-03, ...,\n",
       "          1.19547965e-03,  2.57375580e-03, -8.34247257e-05],\n",
       "        [-8.24936084e-04, -3.88280780e-04, -1.89233315e-03, ...,\n",
       "          1.49454584e-03,  2.83190305e-03,  1.75575362e-04],\n",
       "        [-8.66429880e-04, -6.37497578e-04, -1.93513744e-03, ...,\n",
       "          1.79208710e-03,  3.13810771e-03,  4.84559365e-04]],\n",
       "\n",
       "       [[ 1.96383931e-04,  3.06807087e-05, -3.20026011e-04, ...,\n",
       "          1.60722033e-04,  3.85537860e-04, -7.05594794e-05],\n",
       "        [ 6.54379604e-04,  2.66225834e-04, -5.08971279e-04, ...,\n",
       "          2.15351101e-04,  8.24009883e-04, -1.25674960e-05],\n",
       "        [ 7.70594634e-04,  4.41449083e-04, -4.84112941e-04, ...,\n",
       "         -9.18035221e-05,  6.62330829e-04,  8.17095133e-05],\n",
       "        ...,\n",
       "        [ 3.60007834e-05,  7.93912332e-04, -2.61451671e-04, ...,\n",
       "         -5.12086772e-05,  1.77211850e-03,  1.01391773e-03],\n",
       "        [-1.17956675e-04,  3.85020016e-04, -4.36772360e-04, ...,\n",
       "          4.92845022e-04,  2.17982079e-03,  1.18541380e-03],\n",
       "        [-2.45821255e-04, -1.92548760e-05, -6.76647993e-04, ...,\n",
       "          1.03585341e-03,  2.59835063e-03,  1.38705922e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-9.41508915e-05, -8.06735625e-05,  2.05845601e-04, ...,\n",
       "         -1.80684401e-05,  2.82271096e-04, -1.88497157e-04],\n",
       "        [-1.54211259e-04,  8.76701524e-05,  3.21422121e-04, ...,\n",
       "         -2.48455384e-04,  3.60731035e-04, -4.46508639e-04],\n",
       "        [-4.54713096e-04,  1.74893532e-04,  2.64451868e-04, ...,\n",
       "         -4.77657362e-04,  1.07581058e-04, -7.38460047e-04],\n",
       "        ...,\n",
       "        [ 9.77001037e-04, -3.50236864e-04,  7.91513128e-04, ...,\n",
       "         -2.99590337e-03, -1.72726391e-03, -3.79070407e-04],\n",
       "        [ 9.98785719e-04, -6.76683732e-04,  7.18002731e-04, ...,\n",
       "         -3.22204526e-03, -1.76976889e-03, -3.42427928e-04],\n",
       "        [ 4.84852237e-04, -8.58618063e-04,  6.13686512e-04, ...,\n",
       "         -3.27996165e-03, -1.29958626e-03, -2.83010595e-04]],\n",
       "\n",
       "       [[ 1.96383931e-04,  3.06807087e-05, -3.20026011e-04, ...,\n",
       "          1.60722033e-04,  3.85537860e-04, -7.05594794e-05],\n",
       "        [ 1.75632900e-04,  1.04456507e-04, -4.01743717e-04, ...,\n",
       "          2.39885005e-04,  4.75527107e-04,  3.95579089e-04],\n",
       "        [ 1.68380298e-04, -2.33944651e-04, -6.93720300e-04, ...,\n",
       "          2.80085544e-04,  5.05160075e-04,  8.20802641e-04],\n",
       "        ...,\n",
       "        [-4.63186792e-04, -2.44535855e-04,  6.13453798e-04, ...,\n",
       "          3.25462170e-04,  7.10611930e-04,  4.96390334e-04],\n",
       "        [-5.65884227e-04, -4.40641597e-04,  4.82957461e-04, ...,\n",
       "          7.29248219e-04,  1.12582999e-03,  6.77174365e-04],\n",
       "        [-6.25322748e-04, -6.92238158e-04,  2.59840803e-04, ...,\n",
       "          1.15099107e-03,  1.58707774e-03,  9.15826124e-04]],\n",
       "\n",
       "       [[ 1.96383931e-04,  3.06807087e-05, -3.20026011e-04, ...,\n",
       "          1.60722033e-04,  3.85537860e-04, -7.05594794e-05],\n",
       "        [-3.56166347e-05,  1.36615679e-04, -2.92122131e-04, ...,\n",
       "         -1.38587973e-04,  8.90541589e-04, -2.98277329e-04],\n",
       "        [-6.32175652e-05,  2.93490884e-04, -1.23217178e-04, ...,\n",
       "         -6.09770708e-04,  9.99082578e-04, -7.27498496e-04],\n",
       "        ...,\n",
       "        [-1.33000419e-03, -1.16649992e-03, -2.59838911e-04, ...,\n",
       "          5.10199927e-04,  5.65184164e-04, -4.60610812e-04],\n",
       "        [-1.36423786e-03, -1.55463442e-03, -3.57372279e-04, ...,\n",
       "          1.07413030e-03,  1.08022068e-03, -1.38041811e-04],\n",
       "        [-1.35103997e-03, -1.90409215e-03, -5.12063445e-04, ...,\n",
       "          1.60720397e-03,  1.64978195e-03,  2.18385132e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "    \n",
    "# 불러온 데이터를 모델에 넣어보기\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdb6e111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  6144512   \n",
      "                                                                 \n",
      " lstm (LSTM)                 multiple                  20979712  \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               multiple                  33562624  \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  24590049  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,276,897\n",
      "Trainable params: 85,276,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조 확인\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062726d1",
   "metadata": {},
   "source": [
    "모델은 입력 시퀀스의 길이를 모르기 때문에 Output Shape를 특정할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a10a37e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "686/686 [==============================] - 86s 123ms/step - loss: 3.4990\n",
      "Epoch 2/10\n",
      "686/686 [==============================] - 86s 126ms/step - loss: 2.9381\n",
      "Epoch 3/10\n",
      "686/686 [==============================] - 87s 126ms/step - loss: 2.6474\n",
      "Epoch 4/10\n",
      "686/686 [==============================] - 85s 124ms/step - loss: 2.3824\n",
      "Epoch 5/10\n",
      "686/686 [==============================] - 86s 125ms/step - loss: 2.1314\n",
      "Epoch 6/10\n",
      "686/686 [==============================] - 85s 124ms/step - loss: 1.8924\n",
      "Epoch 7/10\n",
      "686/686 [==============================] - 86s 125ms/step - loss: 1.6719\n",
      "Epoch 8/10\n",
      "686/686 [==============================] - 85s 123ms/step - loss: 1.4775\n",
      "Epoch 9/10\n",
      "686/686 [==============================] - 85s 123ms/step - loss: 1.3135\n",
      "Epoch 10/10\n",
      "686/686 [==============================] - 86s 125ms/step - loss: 1.1811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x217028b8c48>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy( \n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b678cf",
   "metadata": {},
   "source": [
    "### **Step 5. 평가하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "041cf058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장 생성 함수 정의\n",
    "#모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=15):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장 만들기\n",
    "    while True: # 루프를 돌면서 init_sentence에 단어를 하나씩 생성\n",
    "        predict = model(test_tensor) \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated # 최종적으로 모델이 생성한 문장을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ca27a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love the way you shake your thing <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f87960",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mjgpu",
   "language": "python",
   "name": "mjgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
